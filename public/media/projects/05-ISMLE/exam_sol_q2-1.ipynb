{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4750d40184b1e01294feffeca13caf68cf250c2fa8b4aad8afe3ec254527361c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 2.1 Predictive Models: Mauna Loa Dataset\n",
    "\n",
    "_In this question we will work with the Mauna Loa dataset, which contains atmospheric carbon dioxide (CO2) concentrations derived from the Scripps Institute of Oceanographyâ€™s continuous monitoring program at Mauna Loa Observatory Hawaii between 1958 and 1993._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1.1 Bayesian Linear Regression\n",
    "\n",
    "_Write down and describe the equations involved in the Bayesian approach to linear regression, whichmay be used to describe an underlying hidden function of time given noisy data points.  You mayassume an independent Gaussian prior over each of the weights and independent noise on each datapoint._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Solution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As used in lecture 2, we can easily write down the mathematical form the of the Bayesian linear regression\n",
    "\n",
    "\\begin{equation}\n",
    "    y = \\omega X^{\\top} + \\epsilon\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\text{e.g. } \\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^{2})\n",
    "\\end{equation}\n",
    "\n",
    "The likelihood function then follows the following form according to Bishop\n",
    "\n",
    "\\begin{equation}\n",
    "    p(y | X, \\omega) = \\mathcal{N}(\\omega X_{\\top}, \\frac{1}{\\sigma^{2}}).\n",
    "\\end{equation}\n",
    "\n",
    "The prior distribution for the likelihood is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\omega_{0}) = \\mathcal{N}(m_{0}, S_{0})\n",
    "\\end{equation}\n",
    "\n",
    "and its posterior distribution is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\omega_{i+1} | \\omega_{i}, x_{i}, y_{i}) = \\mathcal{N}(m_{i+1}, S_{i+1})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    S_{i+1} = \\left(S^{-1}_{i} + \\beta x^{\\top}_{i} x_{i} \\right)^{-1}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    m_{i+1} = S_{i+1} \\left( S^{-1}_{i} m_{i} + \\beta x_{i} y_{i} \\right)\n",
    "\\end{equation}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.2.2 Connection between Bayesian Linear Regression and Gaussian Processes\n",
    "\n",
    "_By considering the covariance of the function in (2.1) at two time points, show how we can derive an equivalent kernel function using an inner product of basis functions. Hence show that the Bayesian linear regression approach is equivalent to the Gaussian process approach with a suitable choice of kernel function._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Solution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To begin, note that the following two notations of Bayesian linear regression and Gaussian processes are equivalent\n",
    "\n",
    "\\begin{equation}\n",
    "    y = \\omega^{\\top}X, \\quad \\omega \\sim \\mathcal{N}(0, 1_{\\{ d \\times d \\}}) \\Longleftrightarrow f \\sim \\mathcal{GP}(0, K), \\quad K(x, x') = x^{\\top} x'\n",
    "\\end{equation}\n",
    "\n",
    "where the kernel of the Gaussian process is the linear kernel. So let's assume we do multiple draws from the Bayesian Linear Regression, then we have multiple function values, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\{ y_{i} = X_{i} \\omega \\}_{i \\in \\mathcal{S}}\n",
    "\\end{equation}\n",
    "\n",
    "where thence all points have to lie in the same plane, as determined by $\\omega$. If we now repeat the same procedure with the Gaussian process, we obtain a set\n",
    "\n",
    "\\begin{equation}\n",
    "    \\{ y_{i} \\sim \\mathcal{N}(0, X_{i}X_{i}^{\\top}) \\}_{i \\in \\mathcal{S}'}.\n",
    "\\end{equation}\n",
    "\n",
    "If we now diagonalize covariance Kernel, then we obtain\n",
    "\n",
    "\\begin{equation}\n",
    "    X_{i}X_{i}^{\\top} = V \\Lambda V^{\\top}\n",
    "\\end{equation}\n",
    "\n",
    "where only the first d diagonal entries are non-zero, armed with which we can then rewrite the set of Gaussian process realizations as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\{ y_{i} = V \\Lambda^{\\frac{1}{2}} z \\}_{i \\in \\mathcal{S}'}.\n",
    "\\end{equation}\n",
    "\n",
    "Due to the invariance of the Gaussian distribution under linear transformations, see tutorial 1, we can hence conclude that z can be constrained to be a d-dimensional random variable $\\tilde{z}$\n",
    "\n",
    "\\begin{equation}\n",
    "    f_{i} \\overset{d}{=} X_{i} \\tilde{z}, \\quad \\tilde{z} \\sim \\mathcal{N}(0, 1_{\\{ d \\times d \\}}).\n",
    "\\end{equation}\n",
    "\n",
    "Hence showing the equivalence of the algorithms, when considering Gaussian processes with linear kernels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.2.3 Gaussian Processes\n",
    "\n",
    "_Using a Gaussian process, construct a predictive model of CO2 emissions over the subsequent 20 years. Give full details of any assumptions and equations you make use of when fitting your model to the data._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Solution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import ApproximateGP, GP\n",
    "from gpytorch.mlls import VariationalELBO, AddedLossTerm\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL"
   ]
  },
  {
   "source": [
    "We begin by loading the data into our notebook and instantiating it as Tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../Coursework/CO2_Mauna_Loa_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['months since 1960-01-01', 'co2 ppmv'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['months since 1960-01-01']\n",
    "y = dataset['co2 ppmv']\n",
    "\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "X_tensor = X_tensor.view((len(X_tensor), 1))\n",
    "y_tensor = y_tensor.view((len(y_tensor), 1))\n",
    "\n",
    "\n",
    "train_x, train_y = X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "source": [
    "After which we can now begin to define our model. In this case here we are using a deep Gaussian process for which we have to first define its first layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepGPHiddenLayer(DeepGPLayer):\n",
    "\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=128, mean_type='constant'):\n",
    "        if output_dims is None:\n",
    "            inducing_points = torch.randn(num_inducing, input_dims)\n",
    "            batch_shape = torch.Size([])\n",
    "        else:\n",
    "            inducing_points = torch.randn(output_dims, num_inducing, input_dims)\n",
    "            batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        # Define the variational distribution\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "\n",
    "        # Define the variational strategy\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super(DeepGPHiddenLayer, self).__init__(variational_strategy, input_dims, output_dims)\n",
    "\n",
    "        self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=batch_shape, ard_num_dims=input_dims),\n",
    "            batch_shape=batch_shape, ard_num_dims=None\n",
    "        )\n",
    "        self.linear_layer = nn.Linear(input_dims, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "source": [
    "After which we can then construct the actual deep Gaussian process. Here the forward pass then passes through the hidden layers, which could in theory be many more than the ones used here. But this choice boils down to the amount of data we have available"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_dims = 1\n",
    "\n",
    "class DeepGP(DeepGP):\n",
    "\n",
    "    def __init__(self, train_x_shape):\n",
    "        hidden_layer = DeepGPHiddenLayer(\n",
    "            input_dims=train_x_shape[-1],\n",
    "            output_dims=num_output_dims,\n",
    "            mean_type='linear',\n",
    "        )\n",
    "        last_layer = DeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer.output_dims,\n",
    "            output_dims=None,\n",
    "            mean_type='constant'\n",
    "        )\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.last_layer = last_layer\n",
    "        self.likelihood = GaussianLikelihood()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        hidden_rep1 = self.hidden_layer(inputs)\n",
    "        return self.last_layer(hidden_rep1)\n",
    "    \n",
    "    def predict(self, test_loader):\n",
    "        with torch.no_grad():\n",
    "            mus = []\n",
    "            variances = []\n",
    "            lls = []\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                preds = model.likelihood(model(x_batch))\n",
    "                mus.append(preds.mean)\n",
    "                variances.append(preds.variance)\n",
    "                lls.append(model.likelihood.log_marginal(y_batch, model(x_batch)))\n",
    "        \n",
    "        return torch.cat(mus, dim=-1), torch.cat(variances, dim=-1), torch.cat(lls, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepGP(train_x.shape)"
   ]
  },
  {
   "source": [
    "With which we can now move to the definition of the training loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "num_samples = 50"
   ]
  },
  {
   "source": [
    "Define the optimizer and marginal log-likelihood"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([{'params': model.parameters()},], lr=0.01)\n",
    "mll = DeepApproximateMLL(VariationalELBO(model.likelihood, model, train_x.shape[-2]))"
   ]
  },
  {
   "source": [
    "Where we can now define the actual training loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b998cd8e218f4d7e8e4ea365b4010347"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2254098f971e4c8c939e2156e560e643"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "264ca72b31a5449e93b59695c8498dff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7612c46dc0354e51be0f7d24d064502f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "695ba5e8b42045088d69b0867d551328"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "403b89a707e64ad29bdd95f0a22690c9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96dedd7615a54b73ae185ff86d505be6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdfa383b63d0439aab43291162734846"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18556cfe8b224ff8ab3091698cad409a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "332d40145ece457387b960dce3a075e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Minibatch:   0%|          | 0/449 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ce30db9a4b54f30a3ff6fbed68d56dc"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    minibatch_iter = tqdm.tqdm_notebook(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        with gpytorch.settings.num_likelihood_samples(num_samples):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            minibatch_iter.set_postfix(loss=loss.item())"
   ]
  },
  {
   "source": [
    "With which we can then assess our prediction for the next 20 months"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict_batch = torch.arange(431.5, 451)\n",
    "x_predict_batch = x_predict_batch.view((len(x_predict_batch), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mus = []\n",
    "    variances = []\n",
    "    preds = model.likelihood(model(x_predict_batch))\n",
    "    mus.append(preds.mean)\n",
    "    variances.append(preds.variance)"
   ]
  }
 ]
}